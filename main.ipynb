{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèéÔ∏è F1 Pre-Race Excitement Forecaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí≠ Predictive Task\n",
    "\n",
    "The goal of our project is to build a binary classification model that predicts whether a Formula 1 race will be **‚ÄúExciting‚Äù** (1) or **‚ÄúNot Exciting‚Äù** (0) before the race begins.\n",
    "All predictions must rely solely on pre-race features, such as:\n",
    "\n",
    "- Qualifying results and grid spread\n",
    "- Championship standings (point gaps, title contention)\n",
    "- Circuit characteristics (altitude, layout properties)\n",
    "\n",
    "The ground-truth labels are generated from post-race data using objective thresholds on:\n",
    "- Number of DNFs\n",
    "- Total overtakes\n",
    "\n",
    "A race is labeled Exciting if it falls in the top 25% in either category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula 1 is widely regarded as the pinnacle of motorsport, yet the excitement level of each Grand Prix can vary dramatically. Some races unfold as predictable processions, while others deliver unexpected chaos, dramatic overtakes, and major shifts in championship momentum. This inconsistency makes it difficult for casual viewers to decide which races are worth watching, and even dedicated fans often gravitate toward events that promise high action. Predicting that excitement before a race begins, however, is far from straightforward. The dynamics that shape a race‚Äôs entertainment value ‚Äî grid order, competitive pressure, track layout, and environmental conditions ‚Äî all interact in complex ways that are not immediately obvious.\n",
    "\n",
    "In recent years, the narrative surrounding a race before it happens has become almost as influential as the race itself. Tight championship battles, surprising qualifying results, unusual track environments, or major penalties can all heighten expectations for an unpredictable event. Although excitement is inherently subjective, many measurable factors correlate with how action-packed a race turns out to be. Metrics such as overtakes, retirements, and field spread provide concrete signals of how dynamic a race was. These relationships motivate our central goal: to build a machine learning classifier capable of predicting a race‚Äôs entertainment value *strictly* from ***pre-race information***.\n",
    "\n",
    "To explore this, we draw on historical Formula 1 data spanning 1950‚Äì2024, focusing on features that can be known before lights out ‚Äî including qualifying performance, driver standings, and circuit characteristics. We then generate an objective ‚Äúexcitement‚Äù label using post-race metrics such as DNFs and total overtakes. This allows us to train and evaluate our model without introducing future information into the predictions. By combining pre-race context with well-defined labels, our project investigates whether it is possible to anticipate the entertainment potential of a Grand Prix ahead of time. Such a model can help casual fans choose which races to watch live and may provide broader insight into how pre-race conditions shape the dynamics of Formula 1 competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Research on predicting the entertainment value of sporting events has grown a lot in recent years, especially in football, basketball, and various motorsports. Most of that work, however, focuses on performance outcomes ‚Äî things like predicting the race winner, estimating lap times, or modeling podium finishes. In Formula 1 specifically, there‚Äôs been work using historical timing data to estimate pace differences, applying machine learning to forecast pit strategies, or studying how track layout affects overtaking. These kinds of projects show that many aspects of a race can be anticipated using pre-race information.\n",
    "\n",
    "For Formula 1, several Kaggle notebooks and independent analyses have also looked at how overtakes relate to circuit characteristics, how altitude or temperature impacts power units, and how qualifying gaps influence race-day competitiveness. A lot of these takeaways point toward similar themes: grid variability, championship pressure, and track features all play meaningful roles in shaping how a race plays out ‚Äî which aligns with the features we build into our model.\n",
    "\n",
    "What‚Äôs much less common is trying to classify the overall ‚Äúexcitement‚Äù of a race as its own prediction task. The few attempts that exist usually rely on fan surveys or subjective scoring systems, which can be inconsistent and hard to scale. \n",
    "\n",
    "Our approach differs in two important ways:\n",
    "\n",
    "**Objective Labeling**:\n",
    "- Instead of using opinions or ratings, we define excitement using measurable post-race metrics like DNFs and overtakes. This gives us a consistent, reproducible label.\n",
    "\n",
    "**Strictly Pre-Race Features**:\n",
    "- We only use information available before the race starts, which helps avoid data leakage ‚Äî something not all prior work accounts for.\n",
    "\n",
    "By combining insights from motorsport analytics with this stricter setup, our project pushes into a relatively under-explored space: trying to predict how entertaining a Formula 1 race will be before anyone has turned a wheel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #1 ‚Äî Formula 1 World Championship Dataset\n",
    "\n",
    "Dataset Name: [Formula 1 World Championship (1950‚Äì2024)](https://www.kaggle.com/datasets/rohanrao/formula-1-world-championship-1950-2020)\n",
    "\n",
    "Number of observations: ~80,000+ records across all tables\n",
    "\n",
    "Number of variables: Varies by table (5‚Äì20 columns each)\n",
    "\n",
    "This dataset contains the official historical results from every Formula 1 race, including tables such as races, results, qualifying, driver_standings, circuits, and lap_times. Important variables include race metadata (year, round, circuit), driver finishing positions, qualifying times, lap-by-lap positions, and driver point standings. These variables represent concrete performance and contextual indicators such as competitiveness, grid spread, pace variability, and circuit characteristics. For preprocessing, we filtered the data to modern-era races (year ‚â• 2000), handled null values, merged weather data, and removed irrelevant scheduling columns before feature engineering and label creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #2 ‚Äî Formula 1 Weather Dataset\n",
    "\n",
    "Dataset Name: [Formula 1 Race Weather Info (1950‚Äì2024)](https://www.kaggle.com/datasets/mariyakostyrya/formula-1-weather-info-1950-2024)\n",
    "\n",
    "Number of observations: ~1,500\n",
    "\n",
    "Number of variables: 10\n",
    "\n",
    "This dataset provides race-day weather conditions such as temperature, precipitation, humidity, and wind speed for each Grand Prix. Key variables we use include temperature (float), precipitation (float), and a derived feature is_rain, which captures whether the track was wet. Weather serves as a proxy for external race conditions that can influence excitement, risk-taking, and chaos (e.g., wet races tend to produce more DNFs and overtakes). Preprocessing steps included converting timestamps into year/round identifiers, filling missing values with reasonable defaults, and merging the data into the main race table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #3 ‚Äî Combined Race-Level Dataset (Engineered)\n",
    "\n",
    "Dataset Name: F1 Race-Level Modeling Dataset (Engineered)\n",
    "\n",
    "Link: Locally created from dataset #1 and #2\n",
    "\n",
    "Number of observations:1000‚Äì1100 races (depending on filtering)\n",
    "\n",
    "Number of variables: ~20 engineered features\n",
    "\n",
    "This final dataset is constructed by merging the cleaned race metadata with pre-race features (qualifying spread, grid shake-ups, championship tension) and the weather variables. Additional post-race metrics from results and lap_times are used only to create the ground-truth excitement labels (DNFs ‚Üí chaos score, position changes ‚Üí action score). Preprocessing involved merging multiple CSVs on raceId, calculating new features, filtering to modern seasons, and ensuring no data leakage by keeping all post-race information strictly for label generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & Configuration ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = 'data/'  # Folder where you unzipped the Kaggle dataset\n",
    "START_YEAR = 2000    # We focus on the modern era for data consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Loading ---\n",
    "def load_data():\n",
    "    print(\"Loading Vopani Dataset CSVs...\")\n",
    "    # Load core files with '\\N' handling for nulls\n",
    "    races = pd.read_csv(f'{DATA_PATH}races.csv', na_values='\\\\N')\n",
    "    results = pd.read_csv(f'{DATA_PATH}results.csv', na_values='\\\\N')\n",
    "    status = pd.read_csv(f'{DATA_PATH}status.csv', na_values='\\\\N')\n",
    "    qualifying = pd.read_csv(f'{DATA_PATH}qualifying.csv', na_values='\\\\N')\n",
    "    standings = pd.read_csv(f'{DATA_PATH}driver_standings.csv', na_values='\\\\N')\n",
    "    circuits = pd.read_csv(f'{DATA_PATH}circuits.csv', na_values='\\\\N')\n",
    "    lap_times = pd.read_csv(f'{DATA_PATH}lap_times.csv', na_values='\\\\N')\n",
    "    weather = pd.read_csv(f'{DATA_PATH}weather.csv', na_values='\\\\N')\n",
    "    print(\"Datasets are loaded in...\")\n",
    "\n",
    "    return races, results, status, qualifying, standings, circuits, lap_times, weather\n",
    "\n",
    "# Load the raw dataframes\n",
    "races_df, results_df, status_df, qual_df, stand_df, circuits_df, laps_df, weather_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cleaning & Weather Merging ---\n",
    "def clean_and_merge_weather(races, weather):\n",
    "    print(\"Processing & Merging Weather Data...\")\n",
    "    \n",
    "    # 1. Extract Year from Weather Date\n",
    "    weather['datetime'] = pd.to_datetime(weather['datetime'])\n",
    "    weather['year'] = weather['datetime'].dt.year\n",
    "\n",
    "    # 3. Merge Weather with Official Race List\n",
    "    # We join on ['year', 'round'] to align the weather with the correct Race ID\n",
    "    merged_races = pd.merge(\n",
    "        races, \n",
    "        weather[['year', 'round', 'temperature', 'precipitation']], \n",
    "        on=['year', 'round'], \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 4. Filter for Modern Era (2000+)\n",
    "    # merged_races = merged_races[merged_races['year'] >= START_YEAR].copy()\n",
    "    \n",
    "    # 5. Handle Missing Weather (Missing)\n",
    "    # We fill with defaults (Dry, 20¬∞C) so the model doesn't crash\n",
    "    merged_races['precipitation'] = merged_races['precipitation'].fillna(0)\n",
    "    merged_races['temperature'] = merged_races['temperature'].fillna(20.0)\n",
    "\n",
    "    drop_cols = [\n",
    "        'url', \n",
    "        'fp1_date', 'fp1_time', \n",
    "        'fp2_date', 'fp2_time', \n",
    "        'fp3_date', 'fp3_time', \n",
    "        'quali_date', 'quali_time', \n",
    "        'sprint_date', 'sprint_time'\n",
    "    ]\n",
    "    \n",
    "    # Only drop columns that actually exist in the dataframe to avoid errors\n",
    "    cols_to_drop = [c for c in drop_cols if c in merged_races.columns]\n",
    "    \n",
    "    df_clean = merged_races.drop(columns=cols_to_drop)\n",
    "    \n",
    "    print(f\"Total Races processed: {merged_races.shape[0]}\")\n",
    "    return df_clean\n",
    "\n",
    "# Execute Cleaning\n",
    "main_races = clean_and_merge_weather(races_df, weather_df)\n",
    "# Apply the cleaning\n",
    "\n",
    "# Verify what's left\n",
    "print(\"Columns kept:\", main_races.columns.tolist())\n",
    "main_races.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generating Labels (Y) ---\n",
    "def generate_labels(races, results, status, laps):\n",
    "    print(\"Calculating Excitement Labels...\")\n",
    "    \n",
    "    # counts of unique race id\n",
    "    target_race_ids = races['raceId'].unique()\n",
    "    \n",
    "    # --- A. Calculate CHAOS Score (DNFs) ---\n",
    "    # Chaos is a metric determined by how drivers finished a race out of the participating drivers\n",
    "    # The drivers that finished a race, the more likely that the chaos score is lowered\n",
    "\n",
    "    # Copy from results df to get filtered unique race ids\n",
    "    results_filtered = results[results['raceId'].isin(target_race_ids)].copy()\n",
    "    \n",
    "    # Status IDs 1, 11-19 represent normal finishes. Everything else is a DNF.\n",
    "    # In status.csv, ids 1 is considered a normal finish, ids 11-19 refer to how many laps the leader lapped this driver\n",
    "    # The # of laps that the driver was lapped is id - 10 (e.g 11-10 = +1 laps)\n",
    "    # We will consider all types of finishes as a safe finish\n",
    "    normal_finish_ids = [1] + list(range(11, 20))\n",
    "\n",
    "    # Create a new dataframe column name 'is_dnf' \n",
    "    # Checks if drivers in results_filtered had a normal finish\n",
    "    # If true, the tilde inverts (NOT operator) to false and vice versa\n",
    "    results_filtered['is_dnf'] = ~results_filtered['statusId'].isin(normal_finish_ids)\n",
    "    \n",
    "    # Group results_filtered by the race and sum everything in the 'is_dnf' column\n",
    "    # Rename the column of the resulting dataframe into a chaos score\n",
    "    chaos_df = results_filtered.groupby('raceId')['is_dnf'].sum().reset_index()\n",
    "    chaos_df.rename(columns={'is_dnf': 'chaos_score'}, inplace=True)\n",
    "    \n",
    "    # --- B. Calculate ACTION Score (Overtakes/Position Changes) ---\n",
    "    # Filter laps from laptimes.csv with each unique race ids.\n",
    "    laps_filtered = laps[laps['raceId'].isin(target_race_ids)].copy()\n",
    "    laps_filtered.sort_values(['raceId', 'driverId', 'lap'], inplace=True)\n",
    "    \n",
    "    # Calculate position change from previous lap\n",
    "    # Sort by race, driver, and position\n",
    "    # This line takes a unique driver's position in between laps\n",
    "    # diff() computes the difference of current row to previous row\n",
    "    laps_filtered['pos_change'] = laps_filtered.groupby(['raceId', 'driverId'])['position'].diff().abs()\n",
    "\n",
    "    # Group results by new 'pos_change' column to include only the race and position and sum on the position values\n",
    "    # High values mean more position changes = suggests more action\n",
    "    # Low values mean steady position placement = suggest \"boring\" race\n",
    "    action_df = laps_filtered.groupby('raceId')['pos_change'].sum().reset_index()\n",
    "    action_df.rename(columns={'pos_change': 'action_score'}, inplace=True)\n",
    "    \n",
    "    # --- C. Create Final Label ---\n",
    "    # Merge the previous metrics given each unique race\n",
    "    metrics = races[['raceId']].merge(chaos_df, on='raceId', how='left')\n",
    "    metrics = metrics.merge(action_df, on='raceId', how='left').fillna(0)\n",
    "    \n",
    "    # Define \"Exciting\" as Top 25% in Chaos OR Top 25% in Action\n",
    "    chaos_thresh = metrics['chaos_score'].quantile(0.85)\n",
    "    action_thresh = metrics['action_score'].quantile(0.85)\n",
    "    \n",
    "    metrics['is_exciting'] = (\n",
    "        (metrics['chaos_score'] > chaos_thresh) | \n",
    "        (metrics['action_score'] > action_thresh)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"Thresholds -> Chaos: >{chaos_thresh} DNFs, Action: >{action_thresh} Changes\")\n",
    "    return metrics[['raceId', 'is_exciting']]\n",
    "\n",
    "# Execute Label Generation\n",
    "labels_df = generate_labels(main_races, results_df, status_df, laps_df)\n",
    "print(labels_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_laptime(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):  # already numeric\n",
    "        return x\n",
    "    m, s = x.split(':')\n",
    "    return float(m)*60 + float(s)\n",
    "\n",
    "def clean_laptime(t):\n",
    "    if pd.isna(t):\n",
    "        return np.nan\n",
    "    if t < 30 or t > 600:   # unrealistic for any F1 circuit in any era\n",
    "        return np.nan\n",
    "    return t\n",
    "\n",
    "for col in ['q1','q2','q3']:\n",
    "    qual_df[col] = qual_df[col].apply(convert_laptime)\n",
    "    qual_df[col] = qual_df[col].apply(clean_laptime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge qualifying lap times with year to allow for the computation of future features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_df = qual_df.merge(\n",
    "    races_df[['raceId', 'year', 'round']],\n",
    "    on='raceId',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "qual_df['best_time'] = qual_df[['q3', 'q2', 'q1']].bfill(axis=1).iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Spread Time\n",
    "Grid Spread Time represents the amount of variation in drivers‚Äô qualifying lap times before a race. It is calculated as the standard deviation of all qualifying times for a given event.\n",
    "\n",
    "In other words, it measures how tightly grouped or spread out the drivers were in terms of raw pace during qualifying:\n",
    "\n",
    "- Low Grid Spread Time ‚Üí drivers posted very similar lap times, meaning the field was tightly matched.\n",
    "- High Grid Spread Time ‚Üí lap times varied widely, indicating a larger performance gap between drivers.\n",
    "\n",
    "This feature is relevant because qualifying performance often influences how competitive a race will be. A tightly grouped field may lead to closer battles during the race, while a widely spread field may result in fewer on-track position changes.\n",
    "\n",
    "However, as shown in our KDE analysis, the distribution of Grid Spread Time is very similar for both exciting and non-exciting races. This suggests that Grid Spread Time alone does not strongly differentiate race excitement levels and provides limited predictive power for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- GRID SPREAD ----------\n",
    "grid_spread_time = (\n",
    "    qual_df.groupby(\"raceId\")[\"best_time\"]\n",
    "    .std()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"best_time\": \"grid_spread_time\"})\n",
    ")\n",
    "\n",
    "grid_spread_time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid shake-up\n",
    "Grid shake-up measures how far drivers qualified from their usual season performance. Bigger shake-up means the grid is more unusual. These out-of-position starts typically lead to more overtakes and more unpredictable race behavior, making it a subtle but meaningful predictor of excitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- GRID SHAKE-UP ----------\n",
    "# seasonal qualifying average per driver\n",
    "qual_df = qual_df.sort_values(['year', 'round', 'driverId'])\n",
    "\n",
    "season_avg = (\n",
    "    qual_df.groupby(['year', 'driverId'])['position']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={'position': 'season_avg_position'})\n",
    ")\n",
    "\n",
    "\n",
    "qual_pos = qual_df.merge(season_avg, on=[\"year\", \"driverId\"], how=\"left\")\n",
    "qual_pos[\"shake\"] = (qual_pos[\"position\"] - qual_pos[\"season_avg_position\"]).abs()\n",
    "'''\n",
    "qual_df['season_avg_pos'] = (\n",
    "    qual_df.groupby(['year', 'driverId'])['position']\n",
    "    .transform(lambda x: x.expanding().mean().shift(1))\n",
    ")\n",
    "qual_df['season_avg_pos'] = qual_df['season_avg_pos'].fillna(qual_df['position'])\n",
    "\n",
    "# 4. Calculate the Shake\n",
    "# How far is their current grid slot from their running average?\n",
    "qual_df['shake'] = (qual_df['position'] - qual_df['season_avg_pos']).abs()\n",
    "'''\n",
    "grid_shakeup = (\n",
    "    qual_pos.groupby(\"raceId\")[\"shake\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    "    .rename(columns={\"shake\": \"grid_shakeup\"})\n",
    ")\n",
    "\n",
    "grid_shakeup.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Tension\n",
    "Title tension summarizes how close the championship fight is. While it mainly captures fan hype rather than hard race dynamics, close title battles can increase driver aggression and strategic risk, making it a useful context feature even if the direct correlation with our label is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- TITLE TENSION ----------\n",
    "title_tension = (\n",
    "    stand_df.groupby(\"raceId\")\n",
    "    .apply(lambda df: df.sort_values(\"points\", ascending=False).head(3)[\"points\"].std())\n",
    "    .reset_index(name=\"title_tension\")\n",
    ")\n",
    "\n",
    "title_tension.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Altitude\n",
    "Altitude is a track-level feature that affects engine power, cooling, and drag. Certain high-altitude tracks tend to produce more mechanical issues or unusual pace differences. This makes altitude a small but potentially relevant contextual factor, even though its direct correlation with excitement is mild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- ALTITUDE ----------\n",
    "altitude = circuits_df[[\"circuitId\", \"alt\"]].rename(columns={\"alt\": \"altitude\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pre-race features reflect different aspects of the race weekend: grid spread measures how mixed the starting order is; grid shake-up captures how unusual drivers‚Äô qualifying results are relative to their season; title tension describes the competitive pressure in the championship; and altitude represents circuit-specific environmental difficulty. While none of these features individually guarantee an exciting race, each provides contextual cues that, when combined, contribute meaningful predictive signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- MERGE EVERYTHING ----------\n",
    "eda_df = (\n",
    "    main_races\n",
    "    .merge(labels_df, on=\"raceId\", how=\"left\")\n",
    "    .merge(grid_spread_time, on=\"raceId\", how=\"left\")\n",
    "    .merge(grid_shakeup, on=\"raceId\", how=\"left\")\n",
    "    .merge(title_tension, on=\"raceId\", how=\"left\")\n",
    "    .merge(altitude, on=\"circuitId\", how=\"left\")\n",
    ")\n",
    "\n",
    "eda_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    #\"grid_spread_time\", \n",
    "    \"grid_shakeup\", \"title_tension\", \"precipitation\"\n",
    "    #\"altitude\", \"temperature\", \"precipitation\"\n",
    "]\n",
    "\n",
    "#demonstrates correlation between data and how exciting the race is\n",
    "eda_df[features + [\"is_exciting\"]].corr()[\"is_exciting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(data=eda_df, x=\"grid_spread_time\", hue=\"is_exciting\", fill=True, common_norm=False)\n",
    "plt.title(\"Distribution of Grid Spread for Exciting vs Non-Exciting Races\")\n",
    "plt.xlabel(\"Grid Spread (Std Dev of Qualifying Positions)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.kdeplot(data=eda_df, x=\"grid_shakeup\", hue=\"is_exciting\", fill=True, common_norm=False)\n",
    "plt.title(\"Distribution of Grid Shake-Up for Exciting vs Non-Exciting Races\")\n",
    "plt.xlabel(\"Grid Shake-Up (Mean Deviation from Driver‚Äôs Seasonal Average)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Shake-Up vs. Race Excitement\n",
    "Grid shake-up measures how unusual the starting grid is compared to each driver‚Äôs typical qualifying performance that season. Higher shake-up values mean more drivers are starting far from where they ‚Äúnormally‚Äù qualify (e.g., a top driver starting P12, or a midfield driver unexpectedly lining up P5).\n",
    "\n",
    "The KDE plot below compares the distribution of shake-up between exciting and non-exciting races:\n",
    "\n",
    "- Both groups peak around 2.0‚Äì2.5, meaning most races involve some moderate amount of qualifying variability.\n",
    "\n",
    "- The curve for exciting races is slightly shifted to the right, showing more density between 2.5‚Äì4.0.\n",
    "\n",
    "- This suggests that exciting races tend to have a bit more qualifying chaos ‚Äî cars starting out of position, which naturally leads to more overtaking and more dynamic race scenarios.\n",
    "\n",
    "Even though the difference between the two distributions is subtle, it aligns with Formula 1 intuition: When the grid is mixed up, the race is usually more eventful. However, the heavy overlap also makes it clear that qualifying shake-up is not the sole source of excitement ‚Äî many unpredictable factors on race day (weather, crashes, safety cars, tyre issues) can overshadow what happens on Saturday.\n",
    "\n",
    "Exciting races show a slightly higher grid shake-up distribution, meaning more drivers start out of position compared to their seasonal average ‚Äî a factor that tends to lead to more overtakes and race variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale train and test\n",
    "def scaleDatasets(train_df, test_df):\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit only on training data\n",
    "    scaler.fit(train_df)\n",
    "\n",
    "    # Transform both\n",
    "    train_scaled = scaler.transform(train_df)\n",
    "    test_scaled = scaler.transform(test_df)\n",
    "\n",
    "    return train_scaled, test_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features we don't need from the dataframe and split it into training and test sets\n",
    "#also drop the is_exciting stat from the test datframe\n",
    "def dfSplit(eda_df, features):\n",
    "    feature_df = eda_df[features + [\"is_exciting\"]]\n",
    "    size = len(feature_df)\n",
    "\n",
    "    feature_df = feature_df.sample(frac = 1, random_state = 0)\n",
    "    train_df = feature_df.iloc[:int(size * 0.8)]\n",
    "    train_labels = train_df.pop(\"is_exciting\")\n",
    "    test_df = feature_df.iloc[int(size * 0.8):]\n",
    "    test_labels = test_df.pop(\"is_exciting\")\n",
    "\n",
    "    return train_df, train_labels, test_df, test_labels\n",
    "\n",
    "train_df, train_labels, test_df, test_labels = dfSplit(eda_df, features)\n",
    "train_df, test_df = scaleDatasets(train_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(train_df, train_labels)\n",
    "y_dummy = dummy.predict(test_df)\n",
    "\n",
    "print(\"Dummy baseline report:\")\n",
    "print(classification_report(test_labels, y_dummy, zero_division=0))\n",
    "print(\"Confusion matrix (dummy):\\n\", confusion_matrix(test_labels, y_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_model(X_train, y_train, c = 1):\n",
    "    model = LogisticRegression(class_weight = \"balanced\", max_iter = 1000, C = c)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "logisticModel = train_logistic_model(train_df, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_logistic_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    #print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred, zero_division = 0))\n",
    "\n",
    "test_logistic_model(logisticModel, test_df, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation(features, ind):\n",
    "    #for f in features:\n",
    "    print(f\"Removed feature: {features[ind]}\")\n",
    "    new_features = features.copy()\n",
    "    #new_features.remove(f)\n",
    "    new_features.pop(ind)\n",
    "    train_df, train_labels, test_df, test_labels = dfSplit(eda_df, new_features)\n",
    "    train_df, test_df = scaleDatasets(train_df, test_df)\n",
    "    logisticModel = train_logistic_model(train_df, train_labels)\n",
    "    test_logistic_model(logisticModel, test_df, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation(features, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation(features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation(features, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation(features, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_model(X_train, y_train):\n",
    "    model = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    return model\n",
    "\n",
    "rfModel = train_rf_model(train_df, train_labels)\n",
    "def test_rf_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    #print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "test_rf_model(rfModel, test_df, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svc(X_train, y_train):\n",
    "    model = LinearSVC(class_weight='balanced', max_iter=10000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "svcModel = train_svc(train_df, train_labels)\n",
    "\n",
    "\n",
    "def test_svc(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "test_svc(svcModel, test_df, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
